{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIBO08Tmvw7e07vPvkxNjr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/username06983/geospatial_ETL_ELT/blob/main/data226_assignment6_etl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from airflow import DAG\n",
        "from airflow.models import Variable\n",
        "from airflow.decorators import task\n",
        "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
        "from airflow.hooks.base import BaseHook\n",
        "from airflow.utils.dates import days_ago\n",
        "\n",
        "from datetime import timedelta\n",
        "from datetime import datetime\n",
        "import snowflake.connector"
      ],
      "metadata": {
        "id": "prqPh5soeHtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_snowflake_conn():\n",
        "    hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')\n",
        "    conn = hook.get_conn()\n",
        "    return conn"
      ],
      "metadata": {
        "id": "HPn7gcrLeQWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@task\n",
        "def extract_user_session_channel():\n",
        "    import pandas as pd\n",
        "    url = \"s3://s3-geospatial/readonly/user_session_channel.csv\"\n",
        "\n",
        "    df_channel = pd.read_csv(url, storage_options={\"anon\": True})\n",
        "\n",
        "    if df_channel.empty:\n",
        "        raise ValueError(\"user_session_channel.csv is empty or not found.\")\n",
        "\n",
        "    return df_channel\n"
      ],
      "metadata": {
        "id": "eISv5-6PxoOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@task\n",
        "def extract_session_timestamp():\n",
        "    import pandas as pd\n",
        "    url = \"s3://s3-geospatial/readonly/session_timestamp.csv\"\n",
        "    df_timestamp = pd.read_csv(url, storage_options={\"anon\": True})\n",
        "\n",
        "    if df_timestamp.empty:\n",
        "        raise ValueError(\"session_timestamp.csv is empty or not found.\")\n",
        "\n",
        "    return df_timestamp"
      ],
      "metadata": {
        "id": "B0apnwv9yBSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@task\n",
        "def transform(df_channel, df_timestamp):\n",
        "\n",
        "    dup_channel = df_channel[\"sessionid\"].duplicated().sum()\n",
        "    dup_timestamp = df_timestamp[\"sessionid\"].duplicated().sum()\n",
        "\n",
        "    transform_channel = df_channel\n",
        "    transform_timestamp = df_timestamp\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "RFDkZxFP81lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@task\n",
        "def load():\n",
        "    # Targets\n",
        "    target_channel   = \"RAW.USER_SESSION_CHANNEL\"\n",
        "    target_timestamp = \"RAW.SESSION_TIMESTAMP\"\n",
        "\n",
        "    conn = return_snowflake_conn()\n",
        "    cur = conn.cursor()\n",
        "    try:\n",
        "        cur.execute(\"BEGIN;\")\n",
        "\n",
        "        # Context from Airflow connection extras\n",
        "        extras = (BaseHook.get_connection(\"snowflake_conn\").extra_dejson or {})\n",
        "        wh = extras.get(\"warehouse\")\n",
        "        db = extras.get(\"database\")\n",
        "        if wh:\n",
        "            cur.execute(f\"USE WAREHOUSE {wh}\")\n",
        "        if db:\n",
        "            cur.execute(f\"USE DATABASE {db}\")\n",
        "\n",
        "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS RAW;\")\n",
        "        cur.execute(\"USE SCHEMA RAW;\")\n",
        "\n",
        "        cur.execute(\"\"\"\n",
        "            CREATE OR REPLACE STAGE RAW.BLOB_STAGE\n",
        "            URL = 's3://s3-geospatial/readonly/'\n",
        "            FILE_FORMAT = (TYPE = CSV, SKIP_HEADER = 1, FIELD_OPTIONALLY_ENCLOSED_BY = '\"');\n",
        "        \"\"\")\n",
        "\n",
        "        #Create table\n",
        "        cur.execute(f\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS {target_channel} (\n",
        "                userId INT NOT NULL,\n",
        "                sessionId VARCHAR(32) PRIMARY KEY,\n",
        "                channel VARCHAR(32) DEFAULT 'direct'\n",
        "            );\n",
        "        \"\"\")\n",
        "        cur.execute(f\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS {target_timestamp} (\n",
        "                sessionId VARCHAR(32) PRIMARY KEY,\n",
        "                ts TIMESTAMP\n",
        "            );\n",
        "        \"\"\")\n",
        "\n",
        "\n",
        "        # DELETE\n",
        "        cur.execute(f\"DELETE FROM {target_channel};\")\n",
        "        cur.execute(f\"DELETE FROM {target_timestamp};\")\n",
        "\n",
        "        # COPY INTO\n",
        "        cur.execute(f\"\"\"\n",
        "            COPY INTO {target_channel}\n",
        "            FROM @RAW.BLOB_STAGE/user_session_channel.csv\n",
        "            ON_ERROR = 'ABORT_STATEMENT';\n",
        "        \"\"\")\n",
        "\n",
        "        cur.execute(f\"\"\"\n",
        "            COPY INTO {target_timestamp}\n",
        "            FROM @RAW.BLOB_STAGE/session_timestamp.csv\n",
        "            ON_ERROR = 'ABORT_STATEMENT';\n",
        "        \"\"\")\n",
        "\n",
        "        cur.execute(f\"SELECT COUNT(*) FROM {target_channel};\")\n",
        "        channel_cnt = cur.fetchone()[0]\n",
        "        cur.execute(f\"SELECT COUNT(*) FROM {target_timestamp};\")\n",
        "        ts_cnt = cur.fetchone()[0]\n",
        "\n",
        "        cur.execute(\"COMMIT;\")\n",
        "\n",
        "        return {\"user_session_channel\": channel_cnt, \"session_timestamp\": ts_cnt}\n",
        "\n",
        "    except Exception as e:\n",
        "        cur.execute(\"ROLLBACK;\")\n",
        "        print(\"Error:\", e)\n",
        "        raise\n",
        "    finally:\n",
        "        cur.close()\n",
        "        conn.close()"
      ],
      "metadata": {
        "id": "XS1js05KMH4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with DAG(\n",
        "    dag_id=\"etl_geospatial\",\n",
        "    start_date=datetime(2024, 9, 21),\n",
        "    schedule=\"30 2 * * *\",\n",
        "    catchup=False,\n",
        "    tags=[\"geospatial\", \"snowflake\", \"etl\"],\n",
        ") as dag:\n",
        "    ch = extract_user_session_channel()\n",
        "    ts = extract_session_timestamp()\n",
        "    transform(ch, ts)\n",
        "    load()"
      ],
      "metadata": {
        "id": "My3G407PVO_Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}